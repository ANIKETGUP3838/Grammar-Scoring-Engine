{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN5086xp87sK"
      },
      "outputs": [],
      "source": [
        "!pip install librosa transformers language-tool-python tensorflow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "from transformers import pipeline\n",
        "import language_tool_python\n",
        "\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "8JK3Kkjs881Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_features(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=16000)\n",
        "    features = {\n",
        "        'mfcc': librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40),\n",
        "        'chroma': librosa.feature.chroma_stft(y=y, sr=sr),\n",
        "        'contrast': librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    }\n",
        "    return {k: np.mean(v, axis=1) for k, v in features.items()}"
      ],
      "metadata": {
        "id": "lGnvtLbA9Fqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asr_pipe = pipeline(\"automatic-speech-recognition\",\n",
        "                   model=\"openai/whisper-medium\",\n",
        "                   device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def transcribe_audio(file_path):\n",
        "  return asr_pipe(file_path)[\"text\"]\n"
      ],
      "metadata": {
        "id": "kIb9EQpP9GvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "def analyze_grammar(text):\n",
        "    matches = tool.check(text)\n",
        "    return {\n",
        "        'error_count': len(matches),\n",
        "        'error_types': {type(m.ruleId) for m in matches}\n",
        "    }"
      ],
      "metadata": {
        "id": "6Vd5PGUw9Xqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Combine audio features, text statistics, and grammar metrics\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('audio', StandardScaler(), audio_features_columns),\n",
        "    ('text', StandardScaler(), text_features_columns)\n",
        "])"
      ],
      "metadata": {
        "id": "McZSOeUf9YrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
        "\n",
        "# Audio branch\n",
        "audio_input = Input(shape=(audio_feature_dim,))\n",
        "audio_dense = Dense(128, activation='relu')(audio_input)\n",
        "\n",
        "# Text branch\n",
        "text_input = Input(shape=(text_feature_dim,))\n",
        "text_dense = Dense(128, activation='relu')(text_input)\n",
        "\n",
        "# Fusion\n",
        "merged = Concatenate()([audio_dense, text_dense])\n",
        "output = Dense(1, activation='linear')(merged)\n",
        "\n",
        "model = Model(inputs=[audio_input, text_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "nRrySK_u9bcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "history = model.fit(\n",
        "    [X_train_audio, X_train_text],\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=10),\n",
        "        ReduceLROnPlateau(factor=0.2, patience=5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "MCIXvCNr9fx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "y_pred = model.predict([X_test_audio, X_test_text])\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RÂ²: {r2_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "id": "zF7AhVW19jgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}